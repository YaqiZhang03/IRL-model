{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8997989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import timedelta\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f2d702-fc2f-40de-afea-a7601c0c7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f828eb-767b-46f8-8cd1-0d6a30d7e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_excel('./sample data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785b8d2a-0c9d-47b9-ae24-2189fe97c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_trajectories = []\n",
    "\n",
    "for user_id, user_data in result_df.groupby('user_id'):\n",
    "    trajectory = []\n",
    "\n",
    "    for _, row in user_data.iterrows():\n",
    "        state_action = [row['IS'], row['ES'],row['COM'],row['initiate'],row['self_reply'],row['reply_to_others']]\n",
    "        trajectory.append(state_action)\n",
    "    user_trajectories.append(np.array(trajectory))\n",
    "user_trajectories = [trajectory.astype(int) for trajectory in user_trajectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2307e09c-8d46-4c79-84cf-5bef02793986",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dict={0: [0, 0, 0,0,0,0], 1: [0, 0, 0,0,0,1], 2: [0, 0,0,0,1, 0], 3: [0,0,0,1, 0, 0],4:[0,0,1,0,0,1],5:[0,0,1,0,1,0],6:[0,0,1,1,0,0],7:[0,1,0,0,0,1],8:[0,1,0,0,1,0],9:[0,1,0,1,0,0],10:[1,0,0,0,0,1],11:[1,0,0,0,1,0],12:[1,0,0,1,0,0],13:[0,0,1,0,0,0],14:[0,1,0,0,0,0],15:[1,0,0,0,0,0],16:[0,1,1,0,0,1],17:[1,0,1,0,0,1],18:[1,1,0,0,0,1],19:[1,1,1,0,0,1],20:[0,1,1,0,1,0],21:[1,0,1,0,1,0],22:[1,1,0,0,1,0],23:[1,1,1,0,1,0],24:[0,1,1,1,0,0],25:[1,0,1,1,0,0],26:[1,1,0,1,0,0],27:[1,1,1,1,0,0],28:[0,1,1,0,0,0],29:[1,0,1,0,0,0],30:[1,1,0,0,0,0],31:[1,1,1,0,0,0]}\n",
    "a_dict = a_dict={0: [0, 0, 0], 1: [0, 0, 1], 2: [0, 1, 0], 3: [1, 0, 0]}# actions dictionary\n",
    "feature_matrix=[] # feature matrix preparation\n",
    "for el in s_dict:\n",
    "    array=s_dict[el]\n",
    "    int_array = [ int(s) for s in array ]\n",
    "    feature_matrix.append(int_array)\n",
    "feature_matrix = np.asarray(feature_matrix)   \n",
    "c_dict={}\n",
    "for i in np.arange(len(s_dict)):\n",
    "    c_dict[i]=0\n",
    "\n",
    "n_actions = len(a_dict)\n",
    "n_states = len(s_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2590b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value(policy, n_states, transition_probabilities, reward, discount,threshold=1e-2):\n",
    "    v = np.zeros(n_states)\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        diff = 0\n",
    "        for s in range(n_states):\n",
    "            vs = v[s]\n",
    "            a = policy[s]\n",
    "            v[s] = sum(transition_probabilities[s, a, k] *\n",
    "                       (reward[k] + discount * v[k])\n",
    "                       for k in range(n_states))\n",
    "            diff = max(diff, abs(vs - v[s]))\n",
    "\n",
    "    return v\n",
    "    \n",
    "def optimal_value(n_states, n_actions, transition_probabilities, reward,discount, threshold=1e-2):\n",
    "    v = np.zeros(n_states)\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        diff = 0\n",
    "        for s in range(n_states):\n",
    "            max_v = float(\"-inf\")\n",
    "            for a in range(n_actions):\n",
    "                tp = transition_probabilities[s, a, :]\n",
    "                max_v = max(max_v, np.dot(tp, reward + discount*v))\n",
    "            new_diff = abs(v[s] - max_v)\n",
    "            if new_diff > diff:\n",
    "                diff = new_diff\n",
    "            v[s] = max_v\n",
    "    return v\n",
    "\n",
    "def find_policy(n_states, n_actions, transition_probabilities, reward, discount,threshold=1e-2, v=None, stochastic=True):\n",
    "    if v is None:\n",
    "        v = optimal_value(n_states, n_actions, transition_probabilities,reward,\n",
    "                          discount, threshold)\n",
    "    if stochastic:\n",
    "        Q = np.zeros((n_states, n_actions))\n",
    "        for i in range(n_states):\n",
    "            for j in range(n_actions):\n",
    "                p = transition_probabilities[i, j, :]\n",
    "                Q[i, j] = p.dot(reward + discount*v)\n",
    "        Q -= Q.max(axis=1).reshape((n_states, 1))  # For numerical stability.\n",
    "        Q = np.exp(Q)/np.exp(Q).sum(axis=1).reshape((n_states, 1))\n",
    "        return Q\n",
    "    def _policy(s):\n",
    "        return max(range(n_actions),\n",
    "                   key=lambda a: sum(transition_probabilities[s, a, k] *\n",
    "                                     (reward[k] + discount * v[k])\n",
    "                                     for k in range(n_states)))\n",
    "    policy = np.array([_policy(s) for s in range(n_states)])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e405b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl(feature_matrix, n_actions, discount, transition_probability,\n",
    "        trajectories, epochs, learning_rate):\n",
    "    n_states, d_states = feature_matrix.shape\n",
    "    # Initialise weights.\n",
    "    alpha = rn.uniform(size=(d_states,))\n",
    "    # Calculate the feature expectations \\tilde{phi}.\n",
    "    feature_expectations = find_feature_expectations(feature_matrix, trajectories)\n",
    "    # Gradient descent on alpha.\n",
    "    for i in range(epochs):\n",
    "        # print(\"i: {}\".format(i))\n",
    "        r = feature_matrix.dot(alpha)\n",
    "        expected_svf = find_expected_svf(n_states, r, n_actions, discount, transition_probability, trajectories)\n",
    "        grad = feature_expectations - feature_matrix.T.dot(expected_svf)\n",
    "        alpha += learning_rate * grad\n",
    "    return feature_matrix.dot(alpha).reshape((n_states,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37efb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_svf(n_states, trajectories):\n",
    "    svf = np.zeros(n_states)\n",
    "    for trajectory in trajectories:\n",
    "        #for state, _, _ in trajectory:\n",
    "        for state, _ in trajectory:\n",
    "            svf[state] += 1\n",
    "    svf /= trajectories.shape[0]\n",
    "    return svf\n",
    "\n",
    "def find_feature_expectations(feature_matrix, trajectories):\n",
    "    feature_expectations = np.zeros(feature_matrix.shape[1])\n",
    "    total_states = 0\n",
    "    \n",
    "    for trajectory in trajectories:\n",
    "        for state in trajectory: \n",
    "            # Map the state to an integer index and add the corresponding row to feature_expectations.\n",
    "            state_index = np.argmax(state) \n",
    "            feature_expectations += feature_matrix[state_index]\n",
    "\n",
    "            total_states += 1\n",
    "\n",
    "    # Normalize by the total number of states.\n",
    "    feature_expectations /= total_states\n",
    "\n",
    "    return feature_expectations\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "def find_expected_svf(n_states, r, n_actions, discount, transition_probability, trajectories):\n",
    "    n_trajectories = len(trajectories)\n",
    "    trajectory_length = len(trajectories[0]) \n",
    "    policy = find_policy(n_states, n_actions, transition_probability, r, discount)\n",
    "    start_state_count = np.zeros(n_states)\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        start_state_count[trajectory[0]] += 1\n",
    "\n",
    "    p_start_state = start_state_count / n_trajectories\n",
    "\n",
    "    expected_svf = np.tile(p_start_state, (trajectory_length, 1)).T\n",
    "    for t in range(1, trajectory_length):\n",
    "        expected_svf[:, t] = 0\n",
    "        for i, j, k in product(range(n_states), range(n_actions), range(n_states)):\n",
    "            expected_svf[k, t] += (expected_svf[i, t-1] *\n",
    "                                  policy[i, j] *\n",
    "                                  transition_probability[i, j, k])\n",
    "\n",
    "    return expected_svf.sum(axis=1)\n",
    "\n",
    "def softmax(x1, x2):\n",
    "    max_x = max(x1, x2)\n",
    "    min_x = min(x1, x2)\n",
    "    return max_x + np.log(1 + np.exp(min_x - max_x))\n",
    "\n",
    "def find_policy1(n_states, r, n_actions, discount,\n",
    "                           transition_probability):\n",
    "    V = np.nan_to_num(np.ones((n_states, 1)) * float(\"-inf\"))\n",
    "\n",
    "    diff = np.ones((n_states,))\n",
    "    while (diff > 1e-4).all():  # Iterate until convergence.\n",
    "        new_V = r.copy()\n",
    "        for j in range(n_actions):\n",
    "            for i in range(n_states):\n",
    "                new_V[i] = softmax(new_V[i], r[i] + discount*\n",
    "                    np.sum(transition_probability[i, j, k] * V[k]\n",
    "                           for k in range(n_states)))\n",
    "\n",
    "        # # This seems to diverge, so we z-score it (engineering hack).\n",
    "        new_V = (new_V - new_V.mean())/new_V.std()\n",
    "\n",
    "        diff = abs(V - new_V)\n",
    "        V = new_V\n",
    "\n",
    "    \n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    for i in range(n_states):\n",
    "        for j in range(n_actions):\n",
    "            p = np.array([transition_probability[i, j, k]\n",
    "                          for k in range(n_states)])\n",
    "            Q[i, j] = p.dot(r + discount*V)\n",
    "\n",
    "    # Softmax by row to interpret these values as probabilities.\n",
    "    Q -= Q.max(axis=1).reshape((n_states, 1))  # For numerical stability.\n",
    "    Q = np.exp(Q)/np.exp(Q).sum(axis=1).reshape((n_states, 1))\n",
    "    return Q\n",
    "\n",
    "def expected_value_difference(n_states, n_actions, transition_probability,reward, discount, p_start_state, optimal_value, true_reward):\n",
    "    policy = value_iteration.find_policy(n_states, n_actions,transition_probability, reward, discount)\n",
    "    value = value_iteration.value(policy.argmax(axis=1), n_states,transition_probability, true_reward, discount)\n",
    "    evd = optimal_value.dot(p_start_state) - value.dot(p_start_state)\n",
    "    return evd\n",
    "\n",
    "def compute_tp(state_sequences, n_states, n_actions):\n",
    "    tp = np.zeros([n_states, n_actions, n_states])\n",
    "\n",
    "    for user_trajectory in state_sequences:\n",
    "        for pair in np.arange(len(user_trajectory) - 1):\n",
    "            s = user_trajectory[pair][0]\n",
    "            a = user_trajectory[pair][1]\n",
    "            ns = user_trajectory[pair + 1][0]\n",
    "            tp[s, a, ns] += 1\n",
    "\n",
    "    for c in np.arange(len(tp)):\n",
    "        for a in np.arange(len(tp[c])):\n",
    "            total_transitions = tp[c, a].sum()\n",
    "            if total_transitions > 0:\n",
    "                tp[c, a] /= total_transitions\n",
    "\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3568c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRL parameters\n",
    "discount = 0.9\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "l1 = l2 = 0\n",
    "tp= compute_tp(user_trajectories,n_states,n_actions) # compute transition probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "748cbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results3 = []\n",
    "for users in user_trajectories:\n",
    "    r = irl(feature_matrix, n_actions, discount, tp, user_trajectories, epochs,learning_rate) \n",
    "    all_results3.append(r)\n",
    "all_results = pd.DataFrame(all_results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae8549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_excel('./reward.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
